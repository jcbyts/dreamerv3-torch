defaults:

  logdir: null
  traindir: null
  evaldir: null
  offline_traindir: ''
  offline_evaldir: ''
  seed: 0
  deterministic_run: False
  steps: 1000000
  parallel: False
  eval_every: 10000
  eval_episode_num: 10
  log_every: 500
  reset_every: 0
  device: 'cuda:0'
  compile: True
  precision: 32
  debug: False
  video_pred_log: True

  # Environment
  task: 'dmc_walker_walk'
  size: [64, 64]
  envs: 1
  action_repeat: 2
  time_limit: 1000
  grayscale: False
  prefill: 2500
  reward_EMA: True

  # Model
  dyn_hidden: 512
  dyn_deter: 512
  dyn_stoch: 32
  dyn_discrete: 32
  dyn_rec_depth: 1
  dyn_mean_act: 'none'
  dyn_std_act: 'sigmoid2'
  dyn_min_std: 0.1

  # Hierarchical VAE Configuration
  encoder_type: 'original'        # ['original', 'unet_flat', 'densenet_ladder']
  ladder_levels: 1                # Number of hierarchical levels (1 = vanilla Dreamer)
  kl_balancing: false             # Enable per-level KL balancing with free-bits
  dense_growth: 32                # DenseNet growth rate (filters added per layer)
  dense_block_layers: 4           # Number of dense layers per block
  dense_compression: 0.5          # Channel compression ratio in transitions


  # KL annealing parameters for hierarchical mode
  kl_free_bits_max: 1.0  # Maximum free bits value for annealing
  kl_anneal_steps: 50000 # Number of steps over which to anneal free bits

  # KL annealing and balancing parameters for CNVAE
  kl_beta: 1.0                   # final beta value for KL annealing
  kl_beta_min: 1.0e-4              # minimum beta value
  kl_anneal_portion: 0.3         # portion of training for annealing
  kl_const_portion: 0.0          # portion of training at minimum beta
  kl_balancer: "equal"           # KL balancing scheme: equal, linear, sqrt, square

  # Linear probing evaluation parameters
  linear_probe_every: 20000  # Steps between linear probing evaluations
  linear_probe_episodes_train: 6  # Episodes for training linear probes
  linear_probe_episodes_test: 4   # Episodes for testing linear probes
  linear_probe_max_steps: 300     # Max steps per episode for probing
  linear_probe_log_per_factor: false  # Log per-factor RÂ² scores

  grad_heads: ['decoder', 'reward', 'cont']
  units: 512
  act: 'SiLU'
  norm: True
  encoder:
    {mlp_keys: '$^', cnn_keys: 'image', act: 'SiLU', norm: True, cnn_depth: 32, kernel_size: 4, minres: 4, mlp_layers: 5, mlp_units: 1024, symlog_inputs: True}
  decoder:
    {mlp_keys: '$^', cnn_keys: 'image', act: 'SiLU', norm: True, cnn_depth: 32, kernel_size: 4, minres: 4, mlp_layers: 5, mlp_units: 1024, cnn_sigmoid: False, image_dist: mse, vector_dist: symlog_mse, outscale: 1.0}
  actor:
    {layers: 2, dist: 'normal', entropy: 3.0e-4, unimix_ratio: 0.01, std: 'learned', min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 3.0e-5, eps: 1.0e-5, grad_clip: 100.0, outscale: 1.0}
  critic:
    {layers: 2, dist: 'symlog_disc', slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 3.0e-5, eps: 1.0e-5, grad_clip: 100.0, outscale: 0.0}
  reward_head:
    {layers: 2, dist: 'symlog_disc', loss_scale: 1.0, outscale: 0.0}
  cont_head:
    {layers: 2, loss_scale: 1.0, outscale: 1.0}
  dyn_scale: 0.5
  rep_scale: 0.1
  kl_free: 1.0
  weight_decay: 0.0
  unimix_ratio: 0.01
  initial: 'learned'

  # Training
  batch_size: 8
  batch_length: 64
  train_ratio: 512
  pretrain: 100
  model_lr: 1.0e-4
  opt_eps: 1.0e-5
  grad_clip: 1000
  dataset_size: 1000000
  opt: 'adam'

  # Behavior.
  discount: 0.997
  discount_lambda: 0.95
  imag_horizon: 15
  imag_gradient: 'dynamics'
  imag_gradient_mix: 0.0
  eval_state_mean: False

  # Exploration
  expl_behavior: 'greedy'
  expl_until: 0
  expl_extr_scale: 0.0
  expl_intr_scale: 1.0
  disag_target: 'stoch'
  disag_log: True
  disag_models: 10
  disag_offset: 1
  disag_layers: 4
  disag_units: 400
  disag_action_cond: False

dmc_proprio:
  steps: 5e5
  action_repeat: 2
  envs: 4
  train_ratio: 512
  video_pred_log: false
  encoder: {mlp_keys: '.*', cnn_keys: '$^'}
  decoder: {mlp_keys: '.*', cnn_keys: '$^'}

dmc_vision:
  steps: 1e6
  action_repeat: 2
  envs: 4
  train_ratio: 512
  video_pred_log: true
  encoder: {mlp_keys: '$^', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}

crafter:
  task: crafter_reward
  step: 1e6
  action_repeat: 1
  envs: 1
  train_ratio: 512
  video_pred_log: true
  dyn_hidden: 1024
  dyn_deter: 4096
  units: 1024
  encoder: {mlp_keys: '$^', cnn_keys: 'image', cnn_depth: 96, mlp_layers: 5, mlp_units: 1024}
  decoder: {mlp_keys: '$^', cnn_keys: 'image', cnn_depth: 96, mlp_layers: 5, mlp_units: 1024}
  actor: {layers: 5, dist: 'onehot', std: 'none'}
  value: {layers: 5}
  reward_head: {layers: 5}
  cont_head: {layers: 5}
  imag_gradient: 'reinforce'

atari100k:
  steps: 4e5
  envs: 1
  action_repeat: 4
  train_ratio: 1024
  video_pred_log: true
  eval_episode_num: 100
  actor: {dist: 'onehot', std: 'none'}
  imag_gradient: 'reinforce'
  stickey: False
  lives: unused
  noops: 30
  resize: opencv
  actions: needed
  time_limit: 108000

minecraft:
  task: minecraft_diamond
  step: 1e8
  parallel: True
  envs: 16
  # no eval
  eval_episode_num: 0
  eval_every: 1e4
  action_repeat: 1
  train_ratio: 16
  video_pred_log: true
  dyn_hidden: 1024
  dyn_deter: 4096
  units: 1024
  encoder: {mlp_keys: 'inventory|inventory_max|equipped|health|hunger|breath|obs_reward', cnn_keys: 'image', cnn_depth: 96, mlp_layers: 5, mlp_units: 1024}
  decoder: {mlp_keys: 'inventory|inventory_max|equipped|health|hunger|breath', cnn_keys: 'image', cnn_depth: 96, mlp_layers: 5, mlp_units: 1024}
  actor: {layers: 5, dist: 'onehot', std: 'none'}
  value: {layers: 5}
  reward_head: {layers: 5}
  cont_head: {layers: 5}
  imag_gradient: 'reinforce'
  break_speed: 100.0
  time_limit: 36000

memorymaze:
  steps: 1e8
  action_repeat: 2
  actor: {dist: 'onehot', std: 'none'}
  imag_gradient: 'reinforce'
  task: 'memorymaze_9x9'

vizdoom:
  task: vizdoom_basic
  steps: 1000000
  prefill: 10000
  action_repeat: 1
  envs: 4
  train_ratio: 512
  video_pred_log: true
  eval_episode_num: 10
  actor: {dist: 'onehot', std: 'none'}
  imag_gradient: 'reinforce'
  time_limit: 2100
  encoder: {mlp_keys: '$^', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}

vizdoom_basic:
  task: vizdoom_basic
  steps: 1000000
  prefill: 10000
  action_repeat: 1
  envs: 4
  train_ratio: 512
  video_pred_log: true
  eval_episode_num: 10
  actor: {dist: 'onehot', std: 'none'}
  imag_gradient: 'reinforce'
  time_limit: 300
  encoder: {mlp_keys: '$^', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}

vizdoom_deadly_corridor:
  task: vizdoom_deadly_corridor
  steps: 1000000
  prefill: 10000
  action_repeat: 1
  envs: 4
  train_ratio: 512
  video_pred_log: true
  eval_episode_num: 10
  actor: {dist: 'onehot', std: 'none'}
  imag_gradient: 'reinforce'
  time_limit: 2100
  encoder: {mlp_keys: '$^', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}

vizdoom_defend_center:
  task: vizdoom_defend_the_center
  steps: 1000000
  prefill: 10000
  action_repeat: 1
  envs: 4
  train_ratio: 512
  video_pred_log: true
  eval_episode_num: 10
  actor: {dist: 'onehot', std: 'none'}
  imag_gradient: 'reinforce'
  time_limit: 2100
  encoder: {mlp_keys: '$^', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}

vizdoom_health_gathering:
  task: vizdoom_health_gathering
  steps: 1000000
  prefill: 10000
  action_repeat: 1
  envs: 4
  train_ratio: 512
  video_pred_log: true
  eval_episode_num: 10
  actor: {dist: 'onehot', std: 'none'}
  imag_gradient: 'reinforce'
  time_limit: 2100
  encoder: {mlp_keys: '$^', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}    

debug:
  debug: True
  pretrain: 1
  prefill: 1
  batch_size: 10
  batch_length: 20

# Model Variants
# 1) dreamer: vanilla DreamerV3 with categorical latents and original encoder/decoder
dreamer:
  # ------------ logging ---------------------------------------------
  use_wandb: true
  wandb_entity: 'yateslab'
  wandb_project: 'hdreamer'

  # ------------ choose world model ----------------------------------
  model: flat              # flat RSSM, uses dyn_* below
  # ------------ latent / RSSM ---------------------------------------
  dyn_hidden:   512        # size of hidden MLP in stat heads (flat)
  dyn_deter:    510        # deterministic GRU state
  dyn_stoch:     32        # number of categorical variables
  dyn_discrete:  32        # categories per variable
  dyn_rec_depth: 1         # RSSM recurrent depth
  dyn_mean_act: 'none'
  dyn_std_act:  'sigmoid2'
  dyn_min_std:  0.1
  # ------------ optimiser scales ------------------------------------
  kl_free:      1.0
  dyn_scale:    0.5
  rep_scale:    0.1

# 2) hdreamer: hierarchical DreamerV3 with hWorldModel and hRSSM
hdreamer:
  # ------------ logging ---------------------------------------------
  use_wandb: true
  wandb_entity: 'yateslab'
  wandb_project: 'hdreamer'

  # ------------ choose world model ----------------------------------
  model: h                 # use hWorldModel

  # ------------ hierarchical RSSM block -----------------------------
  rssm:
    h_levels:      3
    h_hidden_dim:  256      # hidden size in stat heads
    h_stoch_dims: [10, 10, 12]   # per-level categorical variables
    h_deter_dims: [170, 170, 170] # per-level GRU chunks (divisible by 3)
    discrete:      32       # categories per variable
    act: 'SiLU'
    norm: true
    up_mode: 'bilinear' # must be 'bilinear' 

  # ------------ encoder configuration (for hConvEncoder) ------------
  encoder:
    depth: 64               # base channel depth
    levels: 3               # number of hierarchical levels (matches rssm.h_levels)
    act: 'SiLU'
    norm: true
    kernel_size: 4
    minres: 4
    grow_channels: false

  # ------------ decoder configuration (for DecoderHead) -------------
  decoder:
    cnn_sigmoid: false

  # ------------ shared parameters -----------------------------------
  units: 512
  act: 'SiLU'
  norm: true
  device: 'cuda:0'
  num_actions: 7           # VizDoom action space size

  # ------------ prediction heads ------------------------------------
  reward_head:
    layers: 2
    dist: 'symlog_disc'
    loss_scale: 1.0
    outscale: 0.0

  cont_head:
    layers: 2
    loss_scale: 1.0
    outscale: 1.0

  grad_heads: ['decoder', 'reward', 'cont']

  # ------------ optimizer configuration -----------------------------
  model_lr: 1.0e-4
  opt_eps: 1.0e-8
  grad_clip: 1000
  weight_decay: 0.0
  opt: 'adam'

  # ------------ KL & scales (applied per-level inside hRSSM) --------
  kl_free:      1.0
  dyn_scale:    0.5
  rep_scale:    0.1
  unimix_ratio: 0.01
  initial: 'learned'
  dyn_mean_act: 'none'
  dyn_std_act: 'sigmoid2'
  dyn_min_std: 0.1